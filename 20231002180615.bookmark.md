# Categories &bullet; rmoff's random ramblings (rmoff.net)

<https://rmoff.net/categories/twelvedaysofsmt/>

## Content

[![rmoff\'s random ramblings](https://rmoff.net/img/logo.jpg){.w2 .h2 .br-100}](https://rmoff.net "Home")

[about](https://talks.rmoff.net/bio){.link .f5 .ml2 .dim .near-white}
[talks](https://talks.rmoff.net){.link .f5 .ml2 .dim .near-white}
[](https://www.youtube.com/c/rmoff){.link .f5 .ml2 .dim .near-white}
[](https://data-folks.masto.host/@rmoff){.link .f5 .ml2 .dim .near-white}
[](https://twitter.com/rmoff/){.link .f5 .ml2 .dim .near-white}
[](https://github.com/rmoff/){.link .f5 .ml2 .dim .near-white}
[](https://www.linkedin.com/in/robinmoffatt/){.link .f5 .ml2 .dim .near-white}
[](https://rmoff.net/index.xml "RSS Feed"){.link .f5 .ml2 .dim .near-white .fas .fa-rss-square}
[](https://rmoff.net/search/ "Search"){.link .f5 .ml2 .dim .near-white .fas .fa-search role="search"}

# TwelveDaysOfSMT {#twelvedaysofsmt .near-white .mt1-ns .f2 .fw3 .mb0 .mt0 .lh-title}

##  {#section .near-white .mt3-l .mb4-l .fw1 .f6 .f3-l .measure-wide-l .center .lh-copy .mt2 .mb3}

[ Jan 4, 2021](https://rmoff.net/2021/01/04/kafka-connect-deep-dive-into-single-message-transforms/){.sans-serif .db .child .w-100 .h-100 .f7 .lh-copy .white .no-underline .br3 .ph3 .pt4 .pt5-l .ttu .tc .bg-black-60}

# [Kafka Connect - Deep Dive into Single Message Transforms](https://rmoff.net/2021/01/04/kafka-connect-deep-dive-into-single-message-transforms/){.color-inherit .dim .link} {#kafka-connect---deep-dive-into-single-message-transforms .f3 .fw1 .mt0 .lh-title}

[KIP-66](https://cwiki.apache.org/confluence/display/KAFKA/KIP-66%3A+Single+Message+Transforms+for+Kafka+Connect) was added in Apache Kafka 0.10.2 and brought new functionality called **Single Message Transforms** (SMT). Using SMT you can modify the data and its characteristics as it passes through Kafka Connect pipeline, without needing additional stream processors. For things like manipulating fields, changing topic names, conditionally dropping messages, and more, SMT are a perfect solution. If you get to things like aggregation, joining streams, and lookups then SMT may not be the best for you and you should head over to Kafka Streams or ksqlDB instead.

[Continue Reading ](https://rmoff.net/2021/01/04/kafka-connect-deep-dive-into-single-message-transforms/){.ba .br3 .pa2 .link}

[ Dec 23, 2020](https://rmoff.net/2020/12/23/twelve-days-of-smt-day-12-community-transformations/){.sans-serif .db .child .w-100 .h-100 .f7 .lh-copy .white .no-underline .br3 .ph3 .pt4 .pt5-l .ttu .tc .bg-black-60}

# [ðŸŽ„ Twelve Days of SMT ðŸŽ„ - Day 12: Community Transformations](https://rmoff.net/2020/12/23/twelve-days-of-smt-day-12-community-transformations/){.color-inherit .dim .link} {#twelve-days-of-smt---day-12-community-transformations .f3 .fw1 .mt0 .lh-title}

Apache Kafka ships with many Single Message Transformations (SMT) included - but the great thing about it being an open API is that people can, and do, write their own transformations. Many of these are shared with the wider community, and in this final installment of the series I'm going to look at some of the transformations written by Jeremy Custenborder and available in kafka-connect-transform-common which can be downloaded and installed from Confluent Hub (or built from source, if you like that kind of thing).

[Continue Reading ](https://rmoff.net/2020/12/23/twelve-days-of-smt-day-12-community-transformations/){.ba .br3 .pa2 .link}

[ Dec 22, 2020](https://rmoff.net/2020/12/22/twelve-days-of-smt-day-11-predicate-and-filter/){.sans-serif .db .child .w-100 .h-100 .f7 .lh-copy .white .no-underline .br3 .ph3 .pt4 .pt5-l .ttu .tc .bg-black-60}

# [ðŸŽ„ Twelve Days of SMT ðŸŽ„ - Day 11: Predicate and Filter](https://rmoff.net/2020/12/22/twelve-days-of-smt-day-11-predicate-and-filter/){.color-inherit .dim .link} {#twelve-days-of-smt---day-11-predicate-and-filter .f3 .fw1 .mt0 .lh-title}

Apache Kafka 2.6 included [KIP-585](https://cwiki.apache.org/confluence/display/KAFKA/KIP-585%3A+Filter+and+Conditional+SMTs) which adds support for defining predicates against which transforms are conditionally executed, as well as a [`Filter`](https://docs.confluent.io/platform/current/connect/transforms/filter-ak.html) Single Message Transform to drop messages - which in combination means that you can conditionally drop messages.

As part of Apache Kafka, Kafka Connect ships with pre-built Single Message Transforms and Predicates, but you can also write you own. The API for each is documented: [`Transformation`](https://kafka.apache.org/26/javadoc/org/apache/kafka/connect/transforms/Transformation.html) / [`Predicate`](https://kafka.apache.org/26/javadoc/index.html?org/apache/kafka/connect/transforms/predicates/Predicate.html). The predicates that ship with Apache Kafka are:

-   `RecordIsTombstone` - The value part of the message is null (denoting a tombstone message)

-   `HasHeaderKey`- Matches if a header exists with the name given

-   `TopicNameMatches` - Matches based on topic

[Continue Reading ](https://rmoff.net/2020/12/22/twelve-days-of-smt-day-11-predicate-and-filter/){.ba .br3 .pa2 .link}

[ Dec 21, 2020](https://rmoff.net/2020/12/21/twelve-days-of-smt-day-10-replacefield/){.sans-serif .db .child .w-100 .h-100 .f7 .lh-copy .white .no-underline .br3 .ph3 .pt4 .pt5-l .ttu .tc .bg-black-60}

# [ðŸŽ„ Twelve Days of SMT ðŸŽ„ - Day 10: ReplaceField](https://rmoff.net/2020/12/21/twelve-days-of-smt-day-10-replacefield/){.color-inherit .dim .link} {#twelve-days-of-smt---day-10-replacefield .f3 .fw1 .mt0 .lh-title}

The [`ReplaceField`](https://docs.confluent.io/platform/current/connect/transforms/replacefield.html) Single Message Transform has three modes of operation on fields of data passing through Kafka Connect:

-   Include **only** the fields specified in the list (`whitelist`)

-   Include all fields **except** the ones specified (`blacklist`)

-   Rename field(s) (`renames`)

[Continue Reading ](https://rmoff.net/2020/12/21/twelve-days-of-smt-day-10-replacefield/){.ba .br3 .pa2 .link}

[ Dec 18, 2020](https://rmoff.net/2020/12/18/twelve-days-of-smt-day-9-cast/){.sans-serif .db .child .w-100 .h-100 .f7 .lh-copy .white .no-underline .br3 .ph3 .pt4 .pt5-l .ttu .tc .bg-black-60}

# [ðŸŽ„ Twelve Days of SMT ðŸŽ„ - Day 9: Cast](https://rmoff.net/2020/12/18/twelve-days-of-smt-day-9-cast/){.color-inherit .dim .link} {#twelve-days-of-smt---day-9-cast .f3 .fw1 .mt0 .lh-title}

The [`Cast`](https://docs.confluent.io/platform/current/connect/transforms/cast.html) Single Message Transform lets you change the data type of fields in a Kafka message, supporting numerics, string, and boolean.

[Continue Reading ](https://rmoff.net/2020/12/18/twelve-days-of-smt-day-9-cast/){.ba .br3 .pa2 .link}

[ Dec 17, 2020](https://rmoff.net/2020/12/17/twelve-days-of-smt-day-8-timestampconverter/){.sans-serif .db .child .w-100 .h-100 .f7 .lh-copy .white .no-underline .br3 .ph3 .pt4 .pt5-l .ttu .tc .bg-black-60}

# [ðŸŽ„ Twelve Days of SMT ðŸŽ„ - Day 8: TimestampConverter](https://rmoff.net/2020/12/17/twelve-days-of-smt-day-8-timestampconverter/){.color-inherit .dim .link} {#twelve-days-of-smt---day-8-timestampconverter .f3 .fw1 .mt0 .lh-title}

The [`TimestampConverter`](https://docs.confluent.io/platform/current/connect/transforms/timestampconverter.html) Single Message Transform lets you work with timestamp fields in Kafka messages. You can convert a string into a native [Timestamp](https://kafka.apache.org/26/javadoc/org/apache/kafka/connect/data/Timestamp.html) type (or [Date](https://kafka.apache.org/26/javadoc/org/apache/kafka/connect/data/Date.html) or [Time](https://kafka.apache.org/26/javadoc/org/apache/kafka/connect/data/Time.html)), as well as Unix epoch - and the same in reverse too.

This is really useful to make sure that data ingested into Kafka is correctly stored as a Timestamp (if it is one), and also enables you to write a Timestamp out to a sink connector in a string format that you choose.

[Continue Reading ](https://rmoff.net/2020/12/17/twelve-days-of-smt-day-8-timestampconverter/){.ba .br3 .pa2 .link}

[ Dec 16, 2020](https://rmoff.net/2020/12/16/twelve-days-of-smt-day-7-timestamprouter/){.sans-serif .db .child .w-100 .h-100 .f7 .lh-copy .white .no-underline .br3 .ph3 .pt4 .pt5-l .ttu .tc .bg-black-60}

# [ðŸŽ„ Twelve Days of SMT ðŸŽ„ - Day 7: TimestampRouter](https://rmoff.net/2020/12/16/twelve-days-of-smt-day-7-timestamprouter/){.color-inherit .dim .link} {#twelve-days-of-smt---day-7-timestamprouter .f3 .fw1 .mt0 .lh-title}

Just like the [`RegExRouter`](/2020/12/11/twelve-days-of-smt-day-4-regexrouter/), the [`TimeStampRouter`](https://docs.confluent.io/platform/current/connect/transforms/timestamprouter.html) can be used to modify the topic name of messages as they pass through Kafka Connect. Since the topic name is usually the basis for the naming of the object to which messages are written in a sink connector, this is a great way to achieve time-based partitioning of those objects if required. For example, instead of streaming messages from Kafka to an Elasticsearch index called `cars`, they can be routed to monthly indices e.g. `cars_2020-10`, `cars_2020-11`, `cars_2020-12`, etc.

The `TimeStampRouter` takes two arguments; the format of the final topic name to generate, and the format of the timestamp to put in the topic name (based on [`SimpleDateFormat`](https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html)).

``` {.rouge .highlight style="background-color: #f8f8f8"}
"transforms"                                     : "addTimestampToTopic",
"transforms.addTimestampToTopic.type"            : "org.apache.kafka.connect.transforms.TimestampRouter",
"transforms.addTimestampToTopic.topic.format"    : "${topic}_${timestamp}",
"transforms.addTimestampToTopic.timestamp.format": "YYYY-MM-dd"
```

[Continue Reading ](https://rmoff.net/2020/12/16/twelve-days-of-smt-day-7-timestamprouter/){.ba .br3 .pa2 .link}

[ Dec 15, 2020](https://rmoff.net/2020/12/15/twelve-days-of-smt-day-6-insertfield-ii/){.sans-serif .db .child .w-100 .h-100 .f7 .lh-copy .white .no-underline .br3 .ph3 .pt4 .pt5-l .ttu .tc .bg-black-60}

# [ðŸŽ„ Twelve Days of SMT ðŸŽ„ - Day 6: InsertField II](https://rmoff.net/2020/12/15/twelve-days-of-smt-day-6-insertfield-ii/){.color-inherit .dim .link} {#twelve-days-of-smt---day-6-insertfield-ii .f3 .fw1 .mt0 .lh-title}

We kicked off this series by seeing on [day 1](/2020/12/08/twelve-days-of-smt-day-1-insertfield-timestamp/) how to use [`InsertField`](https://docs.confluent.io/platform/current/connect/transforms/insertfield.html) to add in the timestamp to a message passing through the Kafka Connect sink connector. Today we'll see how to use the same Single Message Transform to add in a static field value, as well as the name of the Kafka topic, partition, and offset from which the message has been read.

``` {.rouge .highlight style="background-color: #f8f8f8"}
"transforms"                                : "insertStaticField1",
"transforms.insertStaticField1.type"        : "org.apache.kafka.connect.transforms.InsertField$Value",
"transforms.insertStaticField1.static.field": "sourceSystem",
"transforms.insertStaticField1.static.value": "NeverGonna"
```

[Continue Reading ](https://rmoff.net/2020/12/15/twelve-days-of-smt-day-6-insertfield-ii/){.ba .br3 .pa2 .link}

[ Dec 14, 2020](https://rmoff.net/2020/12/14/twelve-days-of-smt-day-5-maskfield/){.sans-serif .db .child .w-100 .h-100 .f7 .lh-copy .white .no-underline .br3 .ph3 .pt4 .pt5-l .ttu .tc .bg-black-60}

# [ðŸŽ„ Twelve Days of SMT ðŸŽ„ - Day 5: MaskField](https://rmoff.net/2020/12/14/twelve-days-of-smt-day-5-maskfield/){.color-inherit .dim .link} {#twelve-days-of-smt---day-5-maskfield .f3 .fw1 .mt0 .lh-title}

If you want to mask fields of data as you ingest from a source into Kafka, or write to a sink from Kafka with Kafka Connect, the [`MaskField`](https://docs.confluent.io/platform/current/connect/transforms/maskfield.html) Single Message Transform is perfect for you. It retains the fields whilst replacing its value.

To use the Single Message Transform you specify the field to mask, and its replacement value. To mask the contents of a field called `cc_num` you would use:

``` {.rouge .highlight style="background-color: #f8f8f8"}
"transforms"                               : "maskCC",
"transforms.maskCC.type"                   : "org.apache.kafka.connect.transforms.MaskField$Value",
"transforms.maskCC.fields"                 : "cc_num",
"transforms.maskCC.replacement"            : "****-****-****-****"
```

[Continue Reading ](https://rmoff.net/2020/12/14/twelve-days-of-smt-day-5-maskfield/){.ba .br3 .pa2 .link}

[ Dec 11, 2020](https://rmoff.net/2020/12/11/twelve-days-of-smt-day-4-regexrouter/){.sans-serif .db .child .w-100 .h-100 .f7 .lh-copy .white .no-underline .br3 .ph3 .pt4 .pt5-l .ttu .tc .bg-black-60}

# [ðŸŽ„ Twelve Days of SMT ðŸŽ„ - Day 4: RegExRouter](https://rmoff.net/2020/12/11/twelve-days-of-smt-day-4-regexrouter/){.color-inherit .dim .link} {#twelve-days-of-smt---day-4-regexrouter .f3 .fw1 .mt0 .lh-title}

If you want to change the topic name to which a source connector writes, or object name that's created on a target by a sink connector, the [`RegExRouter`](https://docs.confluent.io/platform/current/connect/transforms/regexrouter.html) is exactly what you need.

To use the Single Message Transform you specify the pattern in the topic name to match, and its replacement. To drop a prefix of `test-` from a topic you would use:

``` {.rouge .highlight style="background-color: #f8f8f8"}
"transforms"                             : "dropTopicPrefix",
"transforms.dropTopicPrefix.type"        : "org.apache.kafka.connect.transforms.RegexRouter",
"transforms.dropTopicPrefix.regex"       : "test-(.*)",
"transforms.dropTopicPrefix.replacement" : "$1"
```

[Continue Reading ](https://rmoff.net/2020/12/11/twelve-days-of-smt-day-4-regexrouter/){.ba .br3 .pa2 .link}

[ Dec 10, 2020](https://rmoff.net/2020/12/10/twelve-days-of-smt-day-3-flatten/){.sans-serif .db .child .w-100 .h-100 .f7 .lh-copy .white .no-underline .br3 .ph3 .pt4 .pt5-l .ttu .tc .bg-black-60}

# [ðŸŽ„ Twelve Days of SMT ðŸŽ„ - Day 3: Flatten](https://rmoff.net/2020/12/10/twelve-days-of-smt-day-3-flatten/){.color-inherit .dim .link} {#twelve-days-of-smt---day-3-flatten .f3 .fw1 .mt0 .lh-title}

The [`Flatten`](https://docs.confluent.io/platform/current/connect/transforms/flatten.html) Single Message Transform (SMT) is useful when you need to collapse a nested message down to a flat structure.

To use the Single Message Transform you only need to reference it; there's no additional configuration required:

``` {.rouge .highlight style="background-color: #f8f8f8"}
"transforms"                    : "flatten",
"transforms.flatten.type"       : "org.apache.kafka.connect.transforms.Flatten$Value"
```

[Continue Reading ](https://rmoff.net/2020/12/10/twelve-days-of-smt-day-3-flatten/){.ba .br3 .pa2 .link}

[ Dec 9, 2020](https://rmoff.net/2020/12/09/twelve-days-of-smt-day-2-valuetokey-and-extractfield/){.sans-serif .db .child .w-100 .h-100 .f7 .lh-copy .white .no-underline .br3 .ph3 .pt4 .pt5-l .ttu .tc .bg-black-60}

# [ðŸŽ„ Twelve Days of SMT ðŸŽ„ - Day 2: ValueToKey and ExtractField](https://rmoff.net/2020/12/09/twelve-days-of-smt-day-2-valuetokey-and-extractfield/){.color-inherit .dim .link} {#twelve-days-of-smt---day-2-valuetokey-and-extractfield .f3 .fw1 .mt0 .lh-title}

Setting the key of a Kafka message is important as it ensures correct logical processing when consumed across multiple partitions, as well as being a requirement when joining to messages in other topics. When using Kafka Connect the connector may already set the key, which is great. If not, you can use these two Single Message Transforms (SMT) to set it as part of the pipeline based on a field in the value part of the message.

To use the [`ValueToKey`](https://docs.confluent.io/platform/current/connect/transforms/valuetokey.html) Single Message Transform specify the name of the field (`id`) that you want to copy from the value to the key:

``` {.rouge .highlight style="background-color: #f8f8f8"}
"transforms"                    : "copyIdToKey",
"transforms.copyIdToKey.type"   : "org.apache.kafka.connect.transforms.ValueToKey",
"transforms.copyIdToKey.fields" : "id",
```

[Continue Reading ](https://rmoff.net/2020/12/09/twelve-days-of-smt-day-2-valuetokey-and-extractfield/){.ba .br3 .pa2 .link}

[ Dec 8, 2020](https://rmoff.net/2020/12/08/twelve-days-of-smt-day-1-insertfield-timestamp/){.sans-serif .db .child .w-100 .h-100 .f7 .lh-copy .white .no-underline .br3 .ph3 .pt4 .pt5-l .ttu .tc .bg-black-60}

# [ðŸŽ„ Twelve Days of SMT ðŸŽ„ - Day 1: InsertField (timestamp)](https://rmoff.net/2020/12/08/twelve-days-of-smt-day-1-insertfield-timestamp/){.color-inherit .dim .link} {#twelve-days-of-smt---day-1-insertfield-timestamp .f3 .fw1 .mt0 .lh-title}

You can use the [`InsertField`](https://docs.confluent.io/platform/current/connect/transforms/insertfield.html) Single Message Transform (SMT) to add the message timestamp into each message that Kafka Connect sends to a sink.

To use the Single Message Transform specify the name of the field (`timestamp.field`) that you want to add to hold the message timestamp:

``` {.rouge .highlight style="background-color: #f8f8f8"}
"transforms"                         : "insertTS",
"transforms.insertTS.type"           : "org.apache.kafka.connect.transforms.InsertField$Value",
"transforms.insertTS.timestamp.field": "messageTS"
```

[Continue Reading ](https://rmoff.net/2020/12/08/twelve-days-of-smt-day-1-insertfield-timestamp/){.ba .br3 .pa2 .link}

------------------------------------------------------------------------

![Robin Moffatt](/images/2018/05/ksldn18-01.jpg)

[](https://www.youtube.com/c/rmoff) [](https://www.linkedin.com/in/robinmoffatt/) *[Robin Moffatt](https://data-folks.masto.host/@rmoff){rel="me"} is a Principal DevEx Engineer at [Decodable](https://decodable.co). He likes writing about himself in the third person, eating good breakfasts, and drinking good beer.*

[![Story logo](https://rmoff.net/img/story-logo-white.svg "Made with Hugo and Story"){.dib style="width: 1.5rem; height: 1.5rem"}](https://github.com/xaprb/story){.no-underline .near-white}

Â© 2023
