# How We Monitor and Run Kafka at Scale | Splunk (www.splunk.com)

<https://www.splunk.com/en_us/blog/devops/how-we-monitor-and-run-kafka-at-scale.html>

## Description

Learn from our experience with Kafka at scale: what to monitor and alert on, troubleshooting, and capacity planning. Splunk Infrastructure Monitoring offers a dashboard out of the box that shows you the most important Kafka metrics at a glance.

## Content

::: iframe
:::

-   [Splunk Sites](#){.dropdown-toggle .cm-link toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"}
    -   [Answers](https://answers.splunk.com/index.html){target="_blank"}
    -   [Blogs](https://www.splunk.com/en_us/blog){target="_blank"}
    -   [Community](https://www.splunk.com/en_us/community.html){target="_blank"}
    -   [.conf](https://conf.splunk.com){target="_blank"}
    -   [Developers](https://dev.splunk.com){target="_blank"}
    -   [Documentation](https://docs.splunk.com/Documentation){target="_blank"}
    -   [Splunk.com](https://www.splunk.com/){target="_blank"}
    -   [Splunkbase](https://splunkbase.splunk.com/){target="_blank"}
    -   [SplunkLive!](https://splunklive.splunk.com/){target="_blank"}
    -   [Support](https://www.splunk.com/en_us/customer-success.html){target="_blank"}
    -   [Training](https://www.splunk.com/en_us/training.html){target="_blank"}
    -   [User Groups](https://usergroups.splunk.com/){target="_blank"}
    -   [Splunk TV](/en_us/resources/videos.html){target="_blank"}

```{=html}
<!-- -->
```
-   [BLOGS](https://www.splunk.com/en_us/blog){.dropdown-toggle role="button" aria-haspopup="true" aria-expanded="false"}

-   [CATEGORIES](#){.dropdown-toggle toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"}

    Categories

    [](#){#Top_Nav-Close .splunk-icon .icon-expanded-close-btn}

    -   [![](/content/dam/splunk-blogs/images/category/bulletins.svg)Bulletins](/en_us/blog/bulletins.html)
    -   [![](/content/dam/splunk-blogs/images/category/conf-splunklive.svg).conf & .conf Go](/en_us/blog/conf-splunklive.html)
    -   [![](/content/dam/splunk-blogs/images/category/customers.svg)Customers & Community](/en_us/blog/customers.html)
    -   [![](/content/dam/splunk-blogs/images/category/devops.svg)DevOps](/en_us/blog/devops.html)
    -   [![](/content/dam/splunk-blogs/images/category/industries.svg)Industries](/en_us/blog/industries.html)

    ```{=html}
    <!-- -->
    ```
    -   [![](/content/dam/splunk-blogs/images/category/it.svg)IT](/en_us/blog/it.html)
    -   [![](/content/dam/splunk-blogs/images/category/leadership.svg)Leadership](/en_us/blog/leadership.html)
    -   [![](/content/dam/splunk-blogs/images/category/default-category.svg)Learn](/en_us/blog/learn.html)
    -   [![](/content/dam/splunk-blogs/images/category/partners.svg)Partners](/en_us/blog/partners.html)
    -   [![](/content/dam/splunk-blogs/images/category/platform.svg)Platform](/en_us/blog/platform.html)

    ```{=html}
    <!-- -->
    ```
    -   [![](/content/dam/splunk-blogs/images/category/security.svg)Security](/en_us/blog/security.html)
    -   [![](/content/dam/splunk-blogs/images/category/splunk-for-good.svg)Global Impact](/en_us/blog/splunk-for-good.html)
    -   [![](/content/dam/splunk-blogs/images/category/splunklife.svg)Splunk Life](/en_us/blog/splunklife.html)
    -   [![](/content/dam/splunk-blogs/images/category/tips-and-tricks.svg)Tips & Tricks](/en_us/blog/tips-and-tricks.html)

    Partner Solution

    Splunk Cloud on AWS

    Simplify your procurement process and subscribe to Splunk Cloud via the AWS marketplace

    [Learn More](https://aws.amazon.com/marketplace/saas/pp/B06XK299KV?ref=_splunk_awspartnerpage){.splunk-btn .sp-btn-borderless .sp-btn-pink .ga-cta}

-   [AUTHORS](#){.dropdown-toggle toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"}

    Authors

    [](#){#Top_Nav-Close .splunk-icon .icon-expanded-close-btn}

    -   [![](/content/dam/splunk-blogs/images/author-profile-pics/gary-steele-headshot.png.thumb.png)Gary Steele](/en_us/blog/author/garysteele.html)
    -   [![](/content/dam/splunk2/images/photos/leadership/claire-hockin.jpg.thumb.png)Claire Hockin](/en_us/blog/author/chockin.html)
    -   [![](/content/splunk-blogs/en_us/author/sxanthos.thumb.png)Spiros Xanthos](/en_us/blog/author/sxanthos.html)
    -   [![](/content/dam/splunk-blogs/images/author-profile-pics/tomc-headshot.jpeg.thumb.png)Tom Casey](/en_us/blog/author/tomc.html)

    ```{=html}
    <!-- -->
    ```
    -   [![](/content/dam/splunk-blogs/images/petra-jenner-square.png.thumb.png)Petra Jenner](/en_us/blog/author/pjenner.html)
    -   [![](/content/dam/splunk-blogs/images/author-profile-pics/sdavies.jpg.thumb.png)Simon Davies](/en_us/blog/author/sdavies.html)
    -   [![](/content/splunk-blogs/en_us/author/rkovar.thumb.png)Ryan Kovar](/en_us/blog/author/rkovar.html)
    -   [![](/content/dam/splunk-blogs/images/author-profile-pics/kriss_%20deiglmeier_headshot.jpg.thumb.png)Kriss Deiglmeier](/en_us/blog/author/kdeiglmeier.html)

    ```{=html}
    <!-- -->
    ```
    -   [![](/content/dam/splunk-blogs/images/author-profile-pics/browan-headshot.jpg.thumb.png)Bill Rowan](/en_us/blog/author/browan.html)
    -   [![](/content/splunk-blogs/en_us/author/amaraqa.thumb.png)Ammar Maraqa](/en_us/blog/author/amaraqa.html)
    -   [![](/content/splunk-blogs/en_us/author/smorgan.thumb.png)Scott Morgan](/en_us/blog/author/smorgan.html)
    -   [![](/content/dam/splunk-blogs/images/author-profile-pics/sharyl-givens-headshot.jpg.thumb.png)Sharyl Givens](/en_us/blog/author/sgivens.html)
    -   [Browse All Authors\...](/en_us/blog/author.html){.splunk-btn .sp-btn-borderless .sp-btn-pink .ga-cta}

    ```{=html}
    <!-- -->
    ```

    E-book

    The Essential Guide to Machine Data

    Unlock the secrets of machine data with our new guide

    [Get the E-book](https://www.splunk.com/en_us/form/the-essential-guide-to-data.html){.splunk-btn .sp-btn-borderless .sp-btn-pink .ga-cta}

-   [SUBSCRIBE](/en_us/blog/subscribe.html){.dropdown-toggle target="_parent" role="button" aria-haspopup="true" aria-expanded="false"}

```{=html}
<!-- -->
```
-   [](#){.dropdown-toggle .search-icon toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"}
-   [Free Splunk](https://www.splunk.com/en_us/download/splunk-cloud.html){.btn .hidden-xs}

[Free Splunk](https://www.splunk.com/en_us/download/splunk-cloud.html){.free-splunk-mobile}

[BLOGS](https://www.splunk.com/en_us/blog)

CATEGORIES

-   [Bulletins](/en_us/blog/bulletins.html)
-   [.conf & .conf Go](/en_us/blog/conf-splunklive.html)
-   [Customers & Community](/en_us/blog/customers.html)
-   [DevOps](/en_us/blog/devops.html)
-   [Industries](/en_us/blog/industries.html)
-   [IT](/en_us/blog/it.html)
-   [Leadership](/en_us/blog/leadership.html)
-   [Learn](/en_us/blog/learn.html)
-   [Partners](/en_us/blog/partners.html)
-   [Platform](/en_us/blog/platform.html)
-   [Security](/en_us/blog/security.html)
-   [Global Impact](/en_us/blog/splunk-for-good.html)
-   [Splunk Life](/en_us/blog/splunklife.html)
-   [Tips & Tricks](/en_us/blog/tips-and-tricks.html)

AUTHORS

-   [Gary Steele](/en_us/blog/author/garysteele.html)
-   [Claire Hockin](/en_us/blog/author/chockin.html)
-   [Spiros Xanthos](/en_us/blog/author/sxanthos.html)
-   [Tom Casey](/en_us/blog/author/tomc.html)
-   [Petra Jenner](/en_us/blog/author/pjenner.html)
-   [Simon Davies](/en_us/blog/author/sdavies.html)
-   [Ryan Kovar](/en_us/blog/author/rkovar.html)
-   [Kriss Deiglmeier](/en_us/blog/author/kdeiglmeier.html)
-   [Bill Rowan](/en_us/blog/author/browan.html)
-   [Ammar Maraqa](/en_us/blog/author/amaraqa.html)
-   [Scott Morgan](/en_us/blog/author/smorgan.html)
-   [Sharyl Givens](/en_us/blog/author/sgivens.html)
-   [Browse All Authors\...](/en_us/blog/author.html){.splunk-btn .sp-btn-borderless .sp-btn-pink .ga-cta}

[SUBSCRIBE](/en_us/blog/subscribe.html){target="_parent"}

Splunk Sites

-   [Answers](https://answers.splunk.com/index.html){target="_blank"}
-   [Blogs](https://www.splunk.com/en_us/blog){target="_blank"}
-   [Community](https://www.splunk.com/en_us/community.html){target="_blank"}
-   [.conf](https://conf.splunk.com){target="_blank"}
-   [Developers](https://dev.splunk.com){target="_blank"}
-   [Documentation](https://docs.splunk.com/Documentation){target="_blank"}
-   [Splunk.com](https://www.splunk.com/){target="_blank"}
-   [Splunkbase](https://splunkbase.splunk.com/){target="_blank"}
-   [SplunkLive!](https://splunklive.splunk.com/){target="_blank"}
-   [Support](https://www.splunk.com/en_us/customer-success.html){target="_blank"}
-   [Training](https://www.splunk.com/en_us/training.html){target="_blank"}
-   [User Groups](https://usergroups.splunk.com/){target="_blank"}
-   [Splunk TV](/en_us/resources/videos.html){target="_blank"}

DEVOPS

# How We Monitor and Run Kafka at Scale {#how-we-monitor-and-run-kafka-at-scale .splunk2-h2 .blog-title}

Share:

-   [](# "Share on Twitter"){.twitter .button-wrap .social-icon data-title="How We Monitor and Run Kafka at Scale" url="https://www.splunk.com/en_us/blog/devops/how-we-monitor-and-run-kafka-at-scale.html" hashtag="#splunkBlogs"}
-   [](# "Share on Facebook"){.facebook .button-wrap .social-icon data-title="How We Monitor and Run Kafka at Scale" url="https://www.splunk.com/en_us/blog/devops/how-we-monitor-and-run-kafka-at-scale.html"}
-   [](# "Share on LinkedIn"){.linkedin .button-wrap .social-icon data-title="How We Monitor and Run Kafka at Scale" url="https://www.splunk.com/en_us/blog/devops/how-we-monitor-and-run-kafka-at-scale.html"}

[![Splunk](/content/dam/splunk-blogs/images/2018/12/splunklogo.png)](/en_us/blog/author/admin.html)
By [Splunk](/en_us/blog/author/admin.html){.author-name} May 08, 2020

S[plunk Infrastructure Monitoring](https://www.splunk.com/en_us/software/infrastructure-monitoring.html){target="_blank"}Â is used to monitor modern infrastructure, consuming metrics from things like [AWS](https://www.signalfx.com/amazon-rds-monitoring/){target="_blank" rel="noopener"},Â [Docker](https://www.signalfx.com/docker-monitoring/){target="_blank" rel="noopener"}, [Elasticsearch](https://www.signalfx.com/elasticsearch-monitoring/){target="_blank" rel="noopener"},Â andÂ [Kafka](https://www.signalfx.com/kafka-monitoring/){target="_blank" rel="noopener"}, and applying analytics in real time. We've relied on Kafka for the core of our infrastructure since the beginning.\

Our engineering team has decades of experience with every kind of streaming or messaging platform out there. Kafka's unique ability to combine *high throughput with persistence* made it ideal as the pipeline underlying all of Splunk Infrastructure Monitoring. Throughput is critical to the kind of dataÂ Splunk Infrastructure Monitoring handles: high-volume, high-resolution streaming times series. And persistence lets us smoothly upgrade components, do performance testing (on replayed data), respond to outages, and fix bugs *without losing data*.Â 

Splunk Infrastructure MonitoringÂ does on the order of *70+ billion messages per day* with:

-   27 brokers
-   1000 (approx) active partitions
-   20 (approx) active topics
-   1M+ messages / sec (and growing daily!)\
    Â 

One huge advantage of Kafka has been the fantastic community that's built up around the project, from independent developers to all the people at Confluent. We've found that for every hurdle we've run into, the community has been extremely responsive in either helping us correct some mistaken pattern on our part---or to fix bugs and add roadmap items that address the needs of environments like Splunk Infrastructure Monitoring.

In this post, we'll go over some lessons learned from monitoring and alerting on Kafka in production, at scale, in a demanding environment with *very high* performance expectations.

![Kafka_Monitoring\_-\_Dashboard_Broker](/content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Dashboard_Broker-1-1024x630.png){.aligncenter .wp-image-5145 .size-large width="1024" height="630" srcset="/content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Dashboard_Broker-1-1024x630.png 1024w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Dashboard_Broker-1-300x185.png 300w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Dashboard_Broker-1-500x308.png 500w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Dashboard_Broker-1-546x336.png 546w" sizes="(max-width: 1024px) 100vw, 1024px"}

## Operating Kafka

### Instrumentation: Collecting Metrics

We use a [GenericJMX](https://collectd.org/wiki/index.php/Plugin:GenericJMX){rel="noopener nofollow" target="_blank"} collectd plugin to collect metrics [exposed](https://kafka.apache.org/documentation.html#monitoring){rel="noopener nofollow" target="_blank"} by Kafka via [JMX](https://en.wikipedia.org/wiki/Java_Management_Extensions){rel="noopener nofollow" target="_blank"}, particularly for [brokers and topics](http://docs.signalfx.com/en/latest/integrations/collectd-info.html#using-collectd-metrics){target="_blank" rel="noopener"}. You can also instrument the clients themselves but we haven't found that necessary (so far). But we do do something interesting there: wrap the client in a layer that's instrumented so that we know what services inÂ Splunk Infrastructure Monitoring are producing or consuming messages and of what size. This is a relatively standard Kafka pattern: adding additional functionality on top of the client to expose more capabilities.

If you use collectd and the GenericJMX plugin configured for Kafka,Â Splunk Infrastructure Monitoring provides built-in dashboards displaying the metrics that we've found most useful when running Kafka in production. Since topics are set by you when you set up Kafka, for per topic metrics we provide templates where you can insert your topic names.

  --------------------------- --------------------------------------------- ----------------------------------------
  Bytes In                    Bytes Out                                     Messages In
  Active Controllers          Request Queue                                 Under Replicated Partitions
  Log Flushes                 Log Flush Time in ms                          Log Flush Time in ms - 95th Percentile
  Produce Total Time          Produce Total Time - 99th Percentile          Produce Total Time - Median
  Fetch Consumer Total Time   Fetch Consumer Total Time - 99th Percentile   Fetch Consumer Total Time - Median
  Fetch Follower Total Time   Fetch Follower Total Time - 99th Percentile   Fetch Follower Total Time - Median
  --------------------------- --------------------------------------------- ----------------------------------------

###  Investigation: Log Flush Latency and Under Replicated Partitions

The most important metrics we track are:

-   Log flush latency (95th percentile)
-   Under-replicated partitions
-   Messages in / sec per broker and per topic
-   Bytes in / sec per broker (collected as a system metric from each broker using collectd)
-   Bytes in / sec per topic
-   Bytes / message\
    Â 

We've found log flush latency and under replicated partitions to be the leading indicators that we need to pay attention to what's going on and prepare to investigate a new bug or regression.

Log flush latency is important, because the longer it takes to flush log to disk, the more the pipeline backs up, the worse our latency and throughput. We monitor for changes in the 95th percentile. When this number goes up, even 10ms going to 20ms, end-to-end latency balloons and all ofÂ Splunk Infrastructure Monitoring is affected.Â 

Under-replicated partitions tells us that replication is not going as fast as configured, which adds latency as consumers don't get the data they need until messages are replicated. It also suggests that we are more vulnerable to losing data if we have a master failure.

Changes in these two metrics generally lead us to dive into the other three metrics.

Messages and bytes in tell us how well balanced our traffic is. if there is a change in their standard deviations, we know that a broker(s) is overloaded. Using messages in / sec and bytes in / sec, we derive bytes / msg.

![Kafka_Monitoring\_-\_Bytes_Per_Message](/content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Bytes_Per_Message-1024x366.png){.aligncenter .wp-image-5138 .size-large width="1024" height="366" srcset="/content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Bytes_Per_Message-1024x366.png 1024w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Bytes_Per_Message-300x107.png 300w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Bytes_Per_Message-500x179.png 500w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Bytes_Per_Message-552x197.png 552w" sizes="(max-width: 1024px) 100vw, 1024px"}

Bigger messages have a higher cost in performance, because it takes a proportionally longer time to process those messages and can cause the pipeline to back up. A sudden, positive change in the message size could indicate a bubble in the pipeline or a fault in an upstream component. A trend of larger message sizes over time suggests an unintended architectural change or an undesirable side effect of a change to another service causing it to produce larger messages.

Real example:

-   We found a bug in our code where we'd increased message size unintentionally
-   We were preallocating 256 byte arrays to serialize messages, which was way too large
-   Everything slowed down
-   But we saw the larger message size on the chart
-   Tracking down and reducing the allocation to just what was needed (about 32 bytes) immediately reduced network I/O by 4x\
    Â 

There are other metrics important for monitoring Kafka that don't come from Kafka directly. For example, our messages themselves have timestamps to track approximate latency from producers to consumers. Increases in end-to-end latency between services can indicate a Kafka issue, since it's usually the largest contributor to inter-service latency for Splunk Infrastructure Monitoring. We look forward to timestamps coming to Kafka messages [soon](https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message){rel="noopener nofollow" target="_blank"}!

Here's another example of an operational issue we experience early on:

-   In one of our consumers, the bytes in was high but it was barely getting any messages. It was listening to an empty topic.
-   Kafka has a feature, for efficiency, where a consumer sends a request saying "give me messages on this topic" and Kafka will park your request until there is at least a certain amount of data (configurable) to reply back with, which is good batching
-   But if your log was completely empty, it would immediately reply with an empty message which isn't actually empty--we found a bug in Kafka
-   So continuously having this request-response caused the network in for the consumer to be high--because even an empty response can be quite a bit of data on the wire (depending on the number of partitions involved)
-   We filed the [bug report](https://issues.apache.org/jira/browse/KAFKA-3159){rel="noopener nofollow" target="_blank"} and it got fixed fast!\
    Â 

### Alerting: Focusing On Leading Indicators

From our experience over the last two years, we've found that *it's most useful to notify on alerts for the two leading indicators: Log Flush Latency (95P) and Under Replicated Partitions*. And investigation usually leads to something at the broker level. We've never really hit a cluster-level issue, which is a testament to how well Kafka's been designed.

Any under replicated partitions at all constitute a bad thing. So for this we use a simple greater-than-zero threshold against the metric exposed from Kafka.

![Kafka_Monitoring\_-\_Under_Replicated_Partitions\_-\_Detector](/content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Under_Replicated_Partitions_-_Detector-1024x504.png){.aligncenter .wp-image-5139 .size-large width="1024" height="504" srcset="/content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Under_Replicated_Partitions_-_Detector-1024x504.png 1024w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Under_Replicated_Partitions_-_Detector-300x148.png 300w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Under_Replicated_Partitions_-_Detector-500x246.png 500w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Under_Replicated_Partitions_-_Detector-552x272.png 552w" sizes="(max-width: 1024px) 100vw, 1024px"}

Log flush latency is a little more complicated. Because some topics are more or less latency sensitive, we set different alert conditions on a per topic basis. Each broker's metrics have metadata that we apply (as key value pairs of property:value) to identify the topics impacted.

For example: raw customer data being ingested is highly latency sensitive, so it gets a 100ms threshold.

![Kafka_Monitoring\_-\_Log_Flush_Too_High\_-\_Customer](/content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Log_Flush_Too_High_-_Customer-1024x516.png){.aligncenter .wp-image-5140 .size-large width="1024" height="516" srcset="/content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Log_Flush_Too_High_-_Customer-1024x516.png 1024w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Log_Flush_Too_High_-_Customer-300x151.png 300w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Log_Flush_Too_High_-_Customer-500x252.png 500w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Log_Flush_Too_High_-_Customer-552x278.png 552w" sizes="(max-width: 1024px) 100vw, 1024px"}

But email can wait plenty of time, so the threshold is orders of magnitude higher.

![Kafka_Monitoring\_-\_Log_Flush_Too_HIgh\_-\_Email_1](/content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Log_Flush_Too_HIgh_-_Email_1-1024x516.png){.aligncenter .wp-image-5141 .size-large width="1024" height="516" srcset="/content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Log_Flush_Too_HIgh_-_Email_1-1024x516.png 1024w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Log_Flush_Too_HIgh_-_Email_1-300x151.png 300w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Log_Flush_Too_HIgh_-_Email_1-500x252.png 500w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Log_Flush_Too_HIgh_-_Email_1-552x278.png 552w" sizes="(max-width: 1024px) 100vw, 1024px"}

Finally, we also alert on having less active controllers than expected, since this is a clear signal that we have a big problem.

### Scaling and Capacity

Scaling Kafka is *involved*.

The adding capacity part is easy. But re-balancing topics/partitions across brokers can be quite hard. For smaller or simpler setups, Kafka can generate an assignment plan for you that provides even distribution across brokers. Which is fine if your brokers are homogenous and co-located. *This does not work well if your brokers are heterogenous or spread across data centers.* So we manually manage the process. Fortunately, Kafka takes care of the actual movement of data, given the partition to broker assignments. And with the expected addition of rack/region awareness, Kafka will soon allow for this kind of spreading of replicas across racks and regions.

This is where the pain comes in. Say you have a lot of traffic on one topic and are adding capacity for it. The topic partitions have to get spread across the new brokers. Although Kafka currently can do quota-based rate limiting for producing and consuming, that's not a applicable to partition movement.Kafka doesn't have a concept of rate limiting during partition movement. If we try to migrate many partitions, each with a lot of data, it can easily saturate our network. So trying to go as fast as possible can cause migrations to take a very long time and increase the risk of message loss.

This issue will be obviated soon, as we're expecting Kafka's built-in rate-limiting capability to be extended to cover partition data balancing. In the meantime, to reduce migration time and the risks, we end up moving one partition at a time, watching the bytes in/out on the source and target brokers, as well as message loss. We use those metrics to control the pace of rebalancing to minimize message loss and resource starvation, thus minimizing service impact.

Here's the dashboard we observe, with the network in/out in the charts on the top right.

![Kafka_Monitoring\_-\_Rebalancing_1](/content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Rebalancing_1-1024x714.png){.aligncenter .wp-image-5142 .size-large width="1024" height="714" srcset="/content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Rebalancing_1-1024x714.png 1024w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Rebalancing_1-300x209.png 300w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Rebalancing_1-500x349.png 500w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Rebalancing_1-482x336.png 482w" sizes="(max-width: 1024px) 100vw, 1024px"}

We'll zoom in on a particular time to see the parallel network in / out activity. Here you'll see that network out on kafka44 is about 220 million bits per second and network in on kafka29 is about 220 million bits per second. You'll also note that we wait for that network activity to go down to baseline before starting the next migration.

![Kafka_Monitoring\_-\_Rebalancing_2](/content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Rebalancing_2.png){.aligncenter .wp-image-5143 .size-full width="475" height="438" srcset="/content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Rebalancing_2.png 475w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Rebalancing_2-300x277.png 300w, /content/dam/splunk-blogs/signalfx-assets/blog-images/Kafka_Monitoring_-_Rebalancing_2-364x336.png 364w" sizes="(max-width: 475px) 100vw, 475px"}

For now, the process is manual. But we're looking forward to completely automating it in the future, like we've begun to do with Elasticsearch. Our approaches to both follow the same general pattern:

-   Baseline the use case to understand performance trade-offs (e.g., learn that small changes to Kafka log flush latency have large impacts on end-to-end service latency)
-   Find the unit of data that can be operated on without causing performance issues
-   Make sure that it's evenly distributed (or accessible as if it were), even if not random
-   Break up large or long running processes that run across those units into smaller components to prevent overload, bottlenecks, and resource starvation so app level performance is not impacted by service level changes (like scaling and rebalancing)\
    Â 

Where scaling is hard, dealing with capacity is quite easy. *Kafka is quite predictably bound by memory and network, not CPU.*

Some heuristics we've learned:

-   Priority is on memory, followed by good disk I/O (use SSDs), and lastly CPU
-   We're on AWS and it just so happens that AMIs with large memory capacity and good disk I/O characteristics have so much CPU that we never even task it If there was an option for a lower CPU, high memory and high disk I/O image, we'd take it.
-   Kafka uses the page cache heavily: you're always writing and consuming from the end of the log, if consumers are keeping up with producers (the normal case)---so the more memory you give it, the more data can stay in page cache. More memory = more cache = less disk access.\
    Â 

Kafka is awesome. Speed and accuracy, in every way, are critical toÂ Splunk Infrastructure Monitoring and the main things our customers depend on. The throughput performance of Kafka is far superior to all the other messaging or [data pipeline platforms](https://www.splunk.com/en_us/blog/learn/data-pipelines.html) we've tested for our use case. Combined with the persistence features, this makes Kafka a core part of our infrastructure.

Splunk Infrastructure MonitoringÂ is excited to have joined the Confluent Partner Program and looking forward to providing Kafka users coordinated solutions that meet the needs of the most demanding environments. As Confluent continues to provide increased visibility into the inner workings of Kafka and greater capabilities through the Confluent Platform,Â Splunk Infrastructure Monitoring will continue to harness that data for deeper monitoring and alerting.

This is a great community to be a part of and we hope everyone who runs Kafka finds this useful. [Check out](https://www.signalfx.com/monitoring-kafka-performance-capacity/?utm_source=blog){target="_blank"} our on-demand webinar about how we run Kafka at more than 70 billion messages per day.\
\
**If you're not already using Splunk Infrastructure Monitoring, get started todayÂ [with our 14-day free trial](https://www.splunk.com/en_us/download/infrastructure-monitoring.html){target="_blank"}.Â **

Thanks,\
Rajiv Kurian

![Splunk](/content/dam/splunk-blogs/images/2018/12/splunklogo.png)

Posted by

#### [Splunk](/en_us/blog/author/admin.html){.author-name .splunk2-color .splunk-black} {#splunk .splunk2-h5}

TAGS

[Observability](/en_us/blog/tag/observability.html){.splunk-btn .sp-btn-black-clear}
[DevOps](/en_us/blog/tag/devops.html){.splunk-btn .sp-btn-black-clear}
[IT](/en_us/blog/tag/it.html){.splunk-btn .sp-btn-black-clear}
[SignalFx](/en_us/blog/tag/signalfx.html){.splunk-btn .sp-btn-black-clear}

Show All Tags

Show Less Tags

#### Related Posts {#related-posts .splunk2-h3 style="text-align:center;"}

[News](https://www.splunk.com/en_us/newsroom.html){target="_blank"}

[Events](https://www.splunk.com/en_us/about-us/events.html){target="_blank "}

Splunk on Twitter

-   [\@Splunk](https://twitter.com/splunk){target="_blank"}
-   [\@splunkanswers](https://twitter.com/splunkanswers){target="_blank"}
-   [\@splunkdev](https://twitter.com/splunkdev){target="_blank"}
-   [\@SplunkUK](https://twitter.com/splunkUK){target="_blank"}
-   [\@SplunkDE](https://twitter.com/splunkDE){target="_blank"}
-   [\@SplunkGov](https://twitter.com/splunkGov){target="_blank"}
-   [\@SplunkforGood](https://twitter.com/splunkforgood){target="_blank"}
-   [\@SplunkDocs](https://twitter.com/splunkdocs){target="_blank"}

Splunk on Facebook

-   [Like us on Facebook](https://www.facebook.com/splunk){target="_blank"}
-   [Like Splunk University on Facebook](https://www.facebook.com/splunkuniversityjobs){target="_blank"}

Splunk on SlideShare

-   [Follow us on SlideShare](https://www.slideshare.net/Splunk){target="_blank"}

Splunk on Instagram

-   [Follow us on Instagram](https://www.instagram.com/splunk/){target="_blank"}

Splunk on LinkedIn

-   [Follow us on LinkedIn](https://www.linkedin.com/company/splunk){target="_blank"}
-   [Follow .conf on LinkedIn](https://www.linkedin.com/showcase/-conf-splunk-worldwide-users%27%E2%80%8B-conference/){target="_blank"}

Splunk on YouTube

-   [Subscribe to our Channel](https://www.youtube.com/user/splunkvideos){target="_blank"}

SPLUNK SITES

-   [Answers](https://community.splunk.com/t5/Splunk-Answers/ct-p/en-us-splunk-answers){target="_blank"}
-   [Community](https://www.splunk.com/en_us/community.html){target="_blank"}
-   [.conf](https://conf.splunk.com/){target="_blank"}
-   [Developers](https://dev.splunk.com/){target="_blank"}
-   [Documentation](https://docs.splunk.com/Documentation){target="_blank"}
-   [Splunk.com](https://www.splunk.com){target="_blank"}
-   [Splunkbase](https://splunkbase.splunk.com/){target="_blank"}
-   [T-Shirt Store](https://www.mypromomall.com/splunk){target="_blank"}
-   [Support](https://www.splunk.com/en_us/customer-success.html){target="_blank"}
-   [Training](https://www.splunk.com/en_us/training.html){target="_blank"}
-   [User Groups](https://usergroups.splunk.com/){target="_blank"}

[](https://twitter.com/splunk){target="_blank"}
[](https://www.facebook.com/splunk){target="_blank"}
[](https://www.linkedin.com/company/splunk){target="_blank"}
[](https://www.instagram.com/splunk/){target="_blank"}
[](https://www.youtube.com/user/splunkvideos){target="_blank"}

Â© 2005-2023 Splunk Inc. All rights reserved.

-   [Sitemap](https://www.splunk.com/en_us/site-map.html){target="_blank"}\|
-   [Contact](https://www.splunk.com/en_us/about-splunk/contact-us.html#tabs/tab_parsys_tabs_OfficeLocations_1){target="_blank"}\|
-   [Careers](https://www.splunk.com/en_us/careers.html){target="_blank"}\|
-   [Privacy](https://www.splunk.com/en_us/legal/privacy/privacy-policy.html){target="_blank"}\|
-   [Terms of Use](https://www.splunk.com/en_us/legal/terms/terms-of-use.html){target="_blank"}\|
-   [Export Control](https://www.splunk.com/en_us/legal/export-controls.html){target="_blank"}\|
-   [Modern Slavery Statement](https://www.splunk.com/en_us/pdfs/legal/splunk-modern-slavery-act-transparancy-statement.pdf){target="_blank"}

Splunk, Splunk\> and Turn Data Into Doing are trademarks or registered trademarks of Splunk Inc. in the United States and other countries. All other brand names, product names, or trademarks belong to their respective owners.
